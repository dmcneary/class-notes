# Concurrrency
February 17, 2022

The idea of concurrency describes doing multiple things (or processes) at once. When multiple CPUs are available, processes may execute in parallel. On a single CPU, concurrency may be achieved by time-sharing.

## Process Interactions
If two processes are run in sequence, data which is shared between them is not at risk of inconsistency. However, when concurrent processes access the same data area, it must be protected from simultaneous change. This area is called a **critical section (CS).**

A software-based solution to the CS problem has a number of requirements:
- **Mutual exclusion**: only one process may be executing the code in the CS
- **Lockout**: the CS is not barred to access by any process which is not currently trying to execute code which it contains
- **Starvation**: while other processes are waiting to access the CS, access cannot be repeatedly prevented by a greedy process
- **Deadlock**: If multiple processes try to access the CS at the same time, they must not block each other forever.

In addition to competition between processes, applications often utilize mutliple processes working together to accomplish a task. Thus, these processes must be allowed to communicate effectively to exchange data between each other.

## Semaphores
A semphore is essentially a gate, or a flag. It allows one process into a CS at a time, based on the state of the semaphore, and removes responsibility for cooperation or competition from individual processes.

The semaphore <code>s</code> is typically implemented as a non-negative integer which can be accessed only by two operations, traditionally called <code>p(s)</code> and <code>v(s)</code>. Based on Dijkstra's Dutch translations describing incrementing and decrementing actions, <code>v(s)</code> will increment the semaphore and <code>p(s)</code> will decrement the semaphore if it is greater than zero. Implementation of these operations must guarantee that simultaneous calls execute sequentially, and a queue of processes waiting to call <code>p(s)</code> will be unblocked one at a time (this can be a random selection, but is usually performed using FIFO).

### The bounded-buffer problem
A classic synchronization problem, this example uses a circular buffer in which a producer and a consumer access slots. The producer makes data slot by slot, which the consumer deletes in the same order. When the buffer is full, the producer must stop and wait for the consumer to free a slot; likewise, if the buffer is empty, the consumer must wait for the producer to fill a slot. Semaphores solve this issue very elegantly by creating counters for filling and emptying slots, <code>f</code> and <code>e</code>. When the producer makes data, it calls <code>p(e)</code> on the empty counter and waits for it to be greater than zero. Upon success of the call, it will place data into the buffer and increment the full counter with <code>v(f)</code>. The consumer follows a similar process, waiting for the full counter to be greater than zero before calling <code>p(f)</code>. It then removes the data from the buffer and calls <code>v(e)</code> to increment.

## Implementing Semaphores in Hardware
Most modern architectures offer efficient solutions to implementing the semaphore design. This is performed using specialized instructions.

A **test-and-set instruction (TS)** copies a variable into a register and sets it to zero. This is performed in a single machine cycle. The instruction is usually in the form of TS(R, x) where R is the register and x is the memory address of the variable. TS allows for easy implementation of a **lock,** which is a synchronization "barrier" through which only one process may pass at a time. The key is to remember that TS always copies the value of x into the register R, *then* sets x to 0.

A **binary semaphore** is just a 0 or 1 value, eliminating the need for consideration of possible integers in the TS instruction. <code>Vb(sb)</code> and <code>Pb(sb)</code> are the operations which set the semaphore to its desired value. **Busy-waiting** is the re-occurance of a while loop when a process is waiting for a change in conditions. CPU resources are consumed even when a process is waiting, since it is still executing <code>while</code> instructions, so implementing <code>Pb(sb)</code> using TS can be wasteful.

Consider a single process *p* which normally accomplishes executing a critical section in *q* quantum units. However, with an additional *n* concurrent processes which time-share the CPU, each process will run for one unit of time during busy-waiting, thus the execution of *p* is increased to *nq* total time units.

A general semaphore may be implemented using a binary semaphore and its operations, as well as a separate semaphore variable. To avoid busy-waiting scenarios, processes will block themselves using s as a resource request.

## Monitors
Semaphore operations are often low-level primitives and are therefore hard to debug. Therefore, **monitors** are implemented as high-level primitives to encapsulate access to the semaphore operations and their data. Essentially, processes are stacked into a queue when the semaphore is closed, and popped off the queue when the semaphore is open. The queueing of processes and the control of semaphore variables are controlled completely by the monitor. Monitors may use a priority queue instead to specify importance rather than using the FIFO behavior of a queue.

## Classic Synchronization Problems
### Readers and Writers
### Dining Philosophers
### Elevator Algorithm
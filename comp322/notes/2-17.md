# Concurrrency
February 17, 2022

The idea of concurrency describes doing multiple things (processes) at once. When multiple CPUs are available, processes may execute in parallel and multiple tasks can be worked on at the same time. On a single CPU, concurrency may be achieved by time-sharing, switching between tasks at a fair rate.

## Process Interactions
If two processes are run in sequence, to completion, data which is shared between them is not at risk of inconsistency. However, when concurrent processes are executed using context switching, and interleaved execution accesses the same data, it must be protected from simultaneous change so that a process does not clobber the changes made by a different process. For instance, two store commands on the same variable by two different processes, executed one after the other in a interleaved manner, would destroy the first process's changes. The area of code is called a **critical section (CS).**

A software-based solution to the issue of critical sections has a number of requirements:
- **Mutual exclusion**: only one process may be executing code in the CS
- **Lockout**: the CS is not barred to access by any process which is not currently trying to execute code which it contains
- **Starvation**: while other processes are waiting to access the CS, access cannot be repeatedly prevented by a greedy process
- **Deadlock**: If multiple processes try to access the CS at the same time, they must not block each other forever.

In addition to competition between processes, applications often utilize mutliple processes working together to accomplish a task. Thus, these processes must be allowed to communicate effectively to exchange data between each other. It is important to distinguish between shared data and a critical section - the data itself is not the CS. Rather, the corresponding code in the processes which access the data are CS's in each process. The **producer/consumer** scenario is common is which two processes cooperate via a shared data structure, such as a buffer. The producer process writes to the buffer and signals that it is full; on the other hand, the consumer reads from a full buffer and marks it empty. Not only can the processes not execute their read and write operations at the same time, but it must be guaranteed that every write operation is followed by a read operation, and vice versa. Otherwise, a write operation on a full buffer would fail at best, and crash the system at worst.

## Semaphores
What if we have more than two processes with CS issues? The software solution presented above must be re-evaluated, but also the waiting process wastes resources by repeatedly testing for access permission. As well, the solution only handles competition among processes. Cooperation takes an entirely different approach.

A semphore is essentially a gate, or a flag. It allows one process into a CS at a time, based on the state of the semaphore, and removes responsibility for cooperation or competition from individual processes.

The semaphore <code>s</code> is typically implemented as a non-negative integer which can be accessed only by two operations, traditionally called <code>p(s)</code> and <code>v(s)</code>. Based on Dijkstra's Dutch translations describing incrementing and decrementing actions, <code>v(s)</code> will increment the semaphore and <code>p(s)</code> will decrement the semaphore if it is greater than zero. Implementation of these operations must guarantee that concurrent calls to increment or decrement only executes in sequence, and a queue of processes waiting to call <code>p(s)</code> will be unblocked one at a time (this can be a random selection, but is usually performed using FIFO).

A single semaphore initialized to 1 can solve the CS problem for any number of processes. This is referred to as a mutex, short for mutual exclusion.

### The bounded-buffer problem
A classic synchronization problem, this example uses a circular buffer in which a producer and a consumer access slots. The producer makes data slot by slot, which the consumer deletes in the same order. When the buffer is full, the producer must stop and wait for the consumer to free a slot; likewise, if the buffer is empty, the consumer must wait for the producer to fill a slot. Semaphores solve this issue very elegantly by creating counters for filling and emptying slots, <code>f</code> and <code>e</code>. When the producer makes data, it calls <code>p(e)</code> on the empty counter and waits for it to be greater than zero. Upon success of the call, it will place data into the buffer and increment the full counter with <code>v(f)</code>. The consumer follows a similar process, waiting for the full counter to be greater than zero before calling <code>p(f)</code>. It then removes the data from the buffer and calls <code>v(e)</code> to increment.